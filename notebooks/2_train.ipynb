{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef0d2692",
   "metadata": {},
   "source": [
    "# Breast Cancer Detection | Train\n",
    "\n",
    "This notebook includes the most important part of the project - the modelling. The notebook tests methodologies for training, and in it the chosen algorithm is decided. Validation also occurs before final testing, which is conducted in the test notebook. This stage is highly iterative, so all model artefacts, logs and configurations are recorded and saved to disk automatically. This initial setup of what will eventually become MLOps for the final product will be really useful, and helps keep track of what is successful and what isn't.\n",
    "\n",
    "Models to try:\n",
    "\n",
    " - [SE-ResNet50](https://github.com/Cadene/pretrained-models.pytorch#senet)\n",
    " - [SE-ResNet101](https://github.com/Cadene/pretrained-models.pytorch#senet)\n",
    " - [SE-ResNet152](https://github.com/Cadene/pretrained-models.pytorch#senet)\n",
    " - [SENet154](https://github.com/Cadene/pretrained-models.pytorch#senet)\n",
    " - [ResNet34](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n",
    " - [ResNet50](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n",
    " - [ResNet101](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n",
    " - [ResNet152](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n",
    " - [FBResNet152](https://github.com/Cadene/pretrained-models.pytorch#facebook-resnet)\n",
    " - [PolyNet](https://github.com/Cadene/pretrained-models.pytorch#polynet)\n",
    " - [InceptionV4](https://github.com/Cadene/pretrained-models.pytorch#inception)\n",
    " - [BNInception](https://github.com/Cadene/pretrained-models.pytorch#bninception)\n",
    " - [InceptionResNetV2](https://github.com/Cadene/pretrained-models.pytorch#inception)\n",
    " - [Xception](https://github.com/Cadene/pretrained-models.pytorch#xception)\n",
    " - [ResNeXt101_32x4d](https://github.com/Cadene/pretrained-models.pytorch#resnext)\n",
    " - [SE-ResNeXt101_32x4d](https://github.com/Cadene/pretrained-models.pytorch#senet)\n",
    " - [DenseNet121](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n",
    " - [DenseNet161](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n",
    " - [DenseNet169](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n",
    " - [DenseNet201](https://github.com/Cadene/pretrained-models.pytorch#torchvision)\n",
    " - [DualPathNet68](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)\n",
    " - [DualPathNet92](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)\n",
    " - [DualPathNet98](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)\n",
    " - [DualPathNet107](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)\n",
    " - [DualPathNet131](https://github.com/Cadene/pretrained-models.pytorch#dualpathnetworks)\n",
    "\n",
    "\n",
    " - efficientnet_b0\n",
    " - efficientnet_b1\n",
    " - efficientnet_b2\n",
    " - efficientnet_b3\n",
    " - efficientnet_b4\n",
    " - efficientnet_b5\n",
    "\n",
    "Initial model work is done by using simple, typical image recognition models (CNN architectures) to see how effective these models can be for the problem. Although I don't expect them to be particularly successful, it's important to establish baselines and take a holistic approach to modelling when it's possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be8b5859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from argparse import ArgumentParser\n",
    "import datetime\n",
    "from time import time\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Data visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn\n",
    "\n",
    "# Image processing\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "# Model evaluation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score, f1_score\n",
    "\n",
    "import torch\n",
    "import pretrainedmodels\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Local imports\n",
    "sys.path.append(\"..\")\n",
    "from utils.dataset import generate_transforms, generate_dataloaders\n",
    "from models.models import *\n",
    "from utils.loss_function import CrossEntropyLossOneHot\n",
    "from utils.lrs_scheduler import WarmRestart, warm_restart\n",
    "from utils.utils import read_images, augs, get_augs, seed_reproducer, init_logger\n",
    "from utils.constants import *\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52a79a8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_name</th>\n",
       "      <th>img_path</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7059b14d2aa03ed6c4de11afa32591995181d31c.jpg</td>\n",
       "      <td>../data/cleaned/none/7059b14d2aa03ed6c4de11afa...</td>\n",
       "      <td>none</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ea1b100b581fcdb7ddfae52cc62347a99e304ba4.jpg</td>\n",
       "      <td>../data/cleaned/none/ea1b100b581fcdb7ddfae52cc...</td>\n",
       "      <td>none</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6eac051b9c45ff6821ec8675216f371711b7cea9.jpg</td>\n",
       "      <td>../data/cleaned/none/6eac051b9c45ff6821ec86752...</td>\n",
       "      <td>none</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fc72767f8520df9b2b83941077dc0ee013eb9399.jpg</td>\n",
       "      <td>../data/cleaned/none/fc72767f8520df9b2b8394107...</td>\n",
       "      <td>none</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49850884a00703afe5aab78c3ce074d2d4acae30.jpg</td>\n",
       "      <td>../data/cleaned/none/49850884a00703afe5aab78c3...</td>\n",
       "      <td>none</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       img_name  \\\n",
       "0  7059b14d2aa03ed6c4de11afa32591995181d31c.jpg   \n",
       "1  ea1b100b581fcdb7ddfae52cc62347a99e304ba4.jpg   \n",
       "2  6eac051b9c45ff6821ec8675216f371711b7cea9.jpg   \n",
       "3  fc72767f8520df9b2b83941077dc0ee013eb9399.jpg   \n",
       "4  49850884a00703afe5aab78c3ce074d2d4acae30.jpg   \n",
       "\n",
       "                                            img_path label  split  \n",
       "0  ../data/cleaned/none/7059b14d2aa03ed6c4de11afa...  none  train  \n",
       "1  ../data/cleaned/none/ea1b100b581fcdb7ddfae52cc...  none  train  \n",
       "2  ../data/cleaned/none/6eac051b9c45ff6821ec86752...  none  train  \n",
       "3  ../data/cleaned/none/fc72767f8520df9b2b8394107...  none  train  \n",
       "4  ../data/cleaned/none/49850884a00703afe5aab78c3...  none  train  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define directories\n",
    "base_dir_path = \"../\"\n",
    "\n",
    "data_dir_path = os.path.join(base_dir_path, \"data\")\n",
    "data_preprocessed_dir_path = os.path.join(data_dir_path, \"preprocessed\")\n",
    "data_preprocessed_train_dir_path = os.path.join(data_dir_path, \"preprocessed/train\")\n",
    "\n",
    "data_dir = os.listdir(data_dir_path)\n",
    "data_preprocessed_dir = os.listdir(data_preprocessed_dir_path)\n",
    "data_preprocessed_train_dir = os.listdir(data_preprocessed_train_dir_path)\n",
    "\n",
    "metadata_preprocessed_path = os.path.join(data_preprocessed_dir_path, \"metadata.csv\")\n",
    "metadata = pd.read_csv(metadata_preprocessed_path)\n",
    "# Subset to train only\n",
    "metadata = metadata.loc[metadata.split == \"train\"]\n",
    "\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4dde582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading images from: ../data/preprocessed/train\n",
      "Rows set to 1024\n",
      "Columns set to 1024\n",
      "Channels set to 3\n",
      "Writing images is set to: False\n",
      "Reading images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 27/27 [00:00<00:00, 48.66it/s]\n",
      "100%|███████████████████████████████████████████| 55/55 [00:02<00:00, 20.74it/s]\n",
      "100%|███████████████████████████████████████████| 21/21 [00:01<00:00, 13.37it/s]\n",
      "100%|███████████████████████████████████████████| 46/46 [00:04<00:00, 10.35it/s]\n",
      "100%|███████████████████████████████████████████| 25/25 [00:03<00:00,  8.27it/s]\n",
      "100%|███████████████████████████████████████████| 21/21 [00:02<00:00,  7.31it/s]\n",
      "100%|███████████████████████████████████████████| 58/58 [00:09<00:00,  6.15it/s]\n",
      "100%|███████████████████████████████████████████| 46/46 [00:09<00:00,  5.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image reading complete.\n",
      "Image array shape: (299, 1024, 1024, 3)\n"
     ]
    }
   ],
   "source": [
    "# Read in train images\n",
    "X_train = read_images(\n",
    "    data_dir_path=data_preprocessed_train_dir_path, \n",
    "    rows=ROWS, \n",
    "    cols=COLS, \n",
    "    channels=CHANNELS, \n",
    "    write_images=False, \n",
    "    output_data_dir_path=None,\n",
    "    verbose=VERBOSE\n",
    ")\n",
    "\n",
    "# Get labels\n",
    "y_train = np.array(pd.get_dummies(metadata[\"label\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9eb1b2",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ac202af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose augmentations to use in preprocessing\n",
    "# For full list see helpers.py\n",
    "#augs_to_select = [\n",
    "#    \"Resize\",\n",
    "#    \"HorizontalFlip\", \n",
    "#    \"VerticalFlip\",\n",
    "#    \"Normalize\"\n",
    "#]\n",
    "## Subset augs based on those selected\n",
    "#AUGS = dict((aug_name, augs[aug_name]) for aug_name in augs_to_select)\n",
    "\n",
    "\n",
    "def init_hparams():\n",
    "    \"\"\"\n",
    "    Initialise hyperparameters for modelling.\n",
    "    \n",
    "    Returns\n",
    "    ---------\n",
    "    hparams : argparse.Namespace\n",
    "        Parsed hyperparameters\n",
    "    \"\"\"\n",
    "    parser = ArgumentParser(add_help=False)\n",
    "    parser.add_argument(\"-backbone\", \"--backbone\", type=str, default=MODEL_NAME)\n",
    "    parser.add_argument(\"-device_name\", type=str, default=DEVICE_NAME)\n",
    "    parser.add_argument(\"--gpus\", default=[0])\n",
    "    parser.add_argument(\"--n_workers\", type=int, default=N_WORKERS)\n",
    "    parser.add_argument(\"--image_size\", nargs=\"+\", default=[ROWS, COLS])\n",
    "    parser.add_argument(\"--seed\", type=int, default=SEED)\n",
    "    parser.add_argument(\"--min_epochs\", type=int, default=MIN_EPOCHS)\n",
    "    parser.add_argument(\"--max_epochs\", type=int, default=MAX_EPOCHS)\n",
    "    parser.add_argument(\"--patience\", type=str, default=PATIENCE)    \n",
    "    parser.add_argument(\"-tbs\", \"--train_batch_size\", type=int, default=TRAIN_BATCH_SIZE)\n",
    "    parser.add_argument(\"-vbs\", \"--val_batch_size\", type=int, default=VAL_BATCH_SIZE)\n",
    "    parser.add_argument(\"--n_splits\", type=int, default=N_SPLITS)\n",
    "    parser.add_argument(\"--test_size\", type=float, default=TEST_SIZE)\n",
    "    parser.add_argument(\"--lr\", type=float, default=LEARNING_RATE)\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=WEIGHT_DECAY)\n",
    "    parser.add_argument(\"--epsilon\", type=float, default=EPSILON)\n",
    "    parser.add_argument(\"--amsgrad\", type=bool, default=AMSGRAD)\n",
    "    parser.add_argument(\"--betas\", default=BETAS)\n",
    "    parser.add_argument(\"--eta_min\", type=float, default=ETA_MIN)\n",
    "    parser.add_argument(\"--t_max\", type=float, default=T_MAX)\n",
    "    parser.add_argument(\"--t_mult\", type=float, default=T_MULT)\n",
    "    parser.add_argument(\"--precision\", type=int, default=PRECISION)\n",
    "    parser.add_argument(\"--gradient_clip_val\", type=float, default=GRADIENT_CLIP_VAL)\n",
    "    parser.add_argument(\"--verbose\", type=str, default=VERBOSE)\n",
    "    parser.add_argument(\"--log_dir\", type=str, default=LOG_DIR)\n",
    "    parser.add_argument(\"--log_name\", type=str, default=LOG_NAME)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        hparams, unknown = parser.parse_known_args()\n",
    "    except:\n",
    "        hparams, unknown = parser.parse_args([])\n",
    "\n",
    "    if len(hparams.gpus) == 1:\n",
    "        hparams.gpus = [int(hparams.gpus[0])]\n",
    "    else:\n",
    "        hparams.gpus = [int(gpu) for gpu in hparams.gpus]\n",
    "\n",
    "    hparams.image_size = [int(size) for size in hparams.image_size]\n",
    "    \n",
    "    return hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca739edd",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b5b9c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoolSystem(pl.LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "\n",
    "        seed_reproducer(self.hparams.seed)\n",
    "\n",
    "        self.model = se_resnext101_32x4d()\n",
    "        self.criterion = CrossEntropyLossOneHot()\n",
    "        self.logger_kun = init_logger(\n",
    "            hparams.log_name, \n",
    "            hparams.log_dir\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.parameters(), \n",
    "            lr=self.hparams.lr, \n",
    "            betas=self.hparams.betas, \n",
    "            eps=self.hparams.epsilon, \n",
    "            weight_decay=self.hparams.weight_decay,\n",
    "            amsgrad=self.hparams.amsgrad\n",
    "        )\n",
    "        self.scheduler = WarmRestart(\n",
    "            self.optimizer, \n",
    "            T_max=self.hparams.t_max, \n",
    "            T_mult=self.hparams.t_mult, \n",
    "            eta_min=self.hparams.eta_min\n",
    "        )\n",
    "        return [self.optimizer], [self.scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        step_start_time = time()\n",
    "        images, labels, data_load_time = batch\n",
    "\n",
    "        scores = self(images)\n",
    "        loss = self.criterion(scores, labels)\n",
    "\n",
    "        data_load_time = torch.sum(data_load_time)\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"data_load_time\": data_load_time,\n",
    "            \"batch_run_time\": torch.Tensor([time() - step_start_time + data_load_time]).to(\n",
    "                data_load_time.device\n",
    "            ),\n",
    "        }\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        # outputs is the return of training_step\n",
    "        train_loss_mean = torch.stack([output[\"loss\"] for output in outputs]).mean()\n",
    "        self.data_load_times = torch.stack([output[\"data_load_time\"] for output in outputs]).sum()\n",
    "        self.batch_run_times = torch.stack([output[\"batch_run_time\"] for output in outputs]).sum()\n",
    "\n",
    "        self.current_epoch += 1\n",
    "        if self.current_epoch < (self.trainer.max_epochs - 4):\n",
    "            self.scheduler = warm_restart(self.scheduler, T_mult=self.hparams.t_mult)\n",
    "\n",
    "        return {\"train_loss\": train_loss_mean}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        step_start_time = time()\n",
    "        images, labels, data_load_time = batch\n",
    "        data_load_time = torch.sum(data_load_time)\n",
    "        scores = self(images)\n",
    "        loss = self.criterion(scores, labels)\n",
    "\n",
    "        # must return key -> val_loss\n",
    "        return {\n",
    "            \"val_loss\": loss,\n",
    "            \"scores\": scores,\n",
    "            \"labels\": labels,\n",
    "            \"data_load_time\": data_load_time,\n",
    "            \"batch_run_time\": torch.Tensor([time() - step_start_time + data_load_time]).to(\n",
    "                data_load_time.device\n",
    "            ),\n",
    "        }\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # compute loss\n",
    "        val_loss_mean = torch.stack([output[\"val_loss\"] for output in outputs]).mean()\n",
    "        self.data_load_times = torch.stack([output[\"data_load_time\"] for output in outputs]).sum()\n",
    "        self.batch_run_times = torch.stack([output[\"batch_run_time\"] for output in outputs]).sum()\n",
    "\n",
    "        # compute roc_auc\n",
    "        scores_all = torch.cat([output[\"scores\"] for output in outputs]).cpu()\n",
    "        labels_all = torch.round(torch.cat([output[\"labels\"] for output in outputs]).cpu())\n",
    "\n",
    "        val_roc_auc = torch.tensor(roc_auc_score(labels_all, scores_all))\n",
    "\n",
    "        # terminal logs\n",
    "        self.logger_kun.info(\n",
    "            f\"{self.hparams.fold_i}-{self.current_epoch} | \"\n",
    "            f\"lr : {self.scheduler.get_lr()[0]:.6f} | \"\n",
    "            f\"val_loss : {val_loss_mean:.4f} | \"\n",
    "            f\"val_roc_auc : {val_roc_auc:.4f} | \"\n",
    "            f\"data_load_times : {self.data_load_times:.2f} | \"\n",
    "            f\"batch_run_times : {self.batch_run_times:.2f}\"\n",
    "        )\n",
    "\n",
    "        return {\"val_loss\": val_loss_mean, \"val_roc_auc\": val_roc_auc}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbd3fde",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c68f6c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-12-07 13:50:48] 866901696.py[  11] : INFO  backbone: se_resnext101_32x4d\n",
      "[2022-12-07 13:50:48] 866901696.py[  12] : INFO  device_name: NVIDIA GeForce RTX 3090\n",
      "[2022-12-07 13:50:48] 866901696.py[  13] : INFO  gpus: [0]\n",
      "[2022-12-07 13:50:48] 866901696.py[  14] : INFO  n_workers: 128\n",
      "[2022-12-07 13:50:48] 866901696.py[  15] : INFO  image_size: [1024, 1024]\n",
      "[2022-12-07 13:50:48] 866901696.py[  16] : INFO  seed: 14\n",
      "[2022-12-07 13:50:48] 866901696.py[  17] : INFO  min_epochs: 75\n",
      "[2022-12-07 13:50:48] 866901696.py[  18] : INFO  max_epochs: 100\n",
      "[2022-12-07 13:50:48] 866901696.py[  19] : INFO  patience: 11\n",
      "[2022-12-07 13:50:48] 866901696.py[  20] : INFO  train_batch_size: 4\n",
      "[2022-12-07 13:50:48] 866901696.py[  21] : INFO  val_batch_size: 4\n",
      "[2022-12-07 13:50:48] 866901696.py[  22] : INFO  n_splits: 3\n",
      "[2022-12-07 13:50:48] 866901696.py[  23] : INFO  test_size: 0.1\n",
      "[2022-12-07 13:50:48] 866901696.py[  24] : INFO  learning rate: 0.0001\n",
      "[2022-12-07 13:50:48] 866901696.py[  25] : INFO  weight_decay: 0\n",
      "[2022-12-07 13:50:48] 866901696.py[  26] : INFO  epsilon: 1e-08\n",
      "[2022-12-07 13:50:48] 866901696.py[  27] : INFO  amsgrad: True\n",
      "[2022-12-07 13:50:48] 866901696.py[  28] : INFO  betas: (0.9, 0.999)\n",
      "[2022-12-07 13:50:48] 866901696.py[  29] : INFO  precision: 16\n",
      "[2022-12-07 13:50:48] 866901696.py[  30] : INFO  gradient_clip_val: 1.0\n",
      "[2022-12-07 13:50:48] 866901696.py[  31] : INFO  eta_min: 5e-06\n",
      "[2022-12-07 13:50:48] 866901696.py[  32] : INFO  t_max: 15\n",
      "[2022-12-07 13:50:48] 866901696.py[  33] : INFO  t_mult: 4\n",
      "[2022-12-07 13:50:48] 866901696.py[  34] : INFO  log_dir: ../logs/logs\n",
      "[2022-12-07 13:50:48] 866901696.py[  35] : INFO  log_name: 2022_12_07_13:50:13\n",
      "[2022-12-07 13:50:48] 866901696.py[  39] : INFO  Notes: t_mult increased to 4 from 2\n",
      "[2022-12-07 13:50:48] 866901696.py[  61] : INFO  Fold 0 num train records: 199\n",
      "[2022-12-07 13:50:48] 866901696.py[  62] : INFO  Fold 0 num val records: 100\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "Using native 16bit precision.\n",
      "[2022-12-07 13:51:36] 1068281035.py[  95] : INFO  0-0 | lr : 0.000100 | val_loss : 2.0566 | val_roc_auc : 0.7366 | data_load_times : 43.76 | batch_run_times : 44.36\n",
      "[2022-12-07 13:52:22] 1068281035.py[  95] : INFO  0-1 | lr : 0.000099 | val_loss : 2.0093 | val_roc_auc : 0.7923 | data_load_times : 44.69 | batch_run_times : 45.48\n",
      "[2022-12-07 13:53:08] 1068281035.py[  95] : INFO  0-2 | lr : 0.000096 | val_loss : 1.9550 | val_roc_auc : 0.7817 | data_load_times : 45.96 | batch_run_times : 46.53\n",
      "[2022-12-07 13:53:54] 1068281035.py[  95] : INFO  0-3 | lr : 0.000091 | val_loss : 1.9358 | val_roc_auc : 0.7703 | data_load_times : 44.85 | batch_run_times : 45.44\n",
      "[2022-12-07 13:54:42] 1068281035.py[  95] : INFO  0-4 | lr : 0.000084 | val_loss : 1.8843 | val_roc_auc : 0.8123 | data_load_times : 45.24 | batch_run_times : 45.79\n",
      "[2022-12-07 13:55:28] 1068281035.py[  95] : INFO  0-5 | lr : 0.000076 | val_loss : 1.8714 | val_roc_auc : 0.7983 | data_load_times : 46.49 | batch_run_times : 47.24\n",
      "[2022-12-07 13:56:14] 1068281035.py[  95] : INFO  0-6 | lr : 0.000067 | val_loss : 1.8210 | val_roc_auc : 0.8223 | data_load_times : 44.93 | batch_run_times : 45.62\n",
      "[2022-12-07 13:57:00] 1068281035.py[  95] : INFO  0-7 | lr : 0.000057 | val_loss : 1.7878 | val_roc_auc : 0.8207 | data_load_times : 45.51 | batch_run_times : 46.06\n",
      "[2022-12-07 13:57:47] 1068281035.py[  95] : INFO  0-8 | lr : 0.000048 | val_loss : 1.7826 | val_roc_auc : 0.8051 | data_load_times : 46.56 | batch_run_times : 47.12\n",
      "[2022-12-07 13:58:34] 1068281035.py[  95] : INFO  0-9 | lr : 0.000038 | val_loss : 1.7437 | val_roc_auc : 0.8281 | data_load_times : 44.95 | batch_run_times : 45.56\n",
      "[2022-12-07 13:59:21] 1068281035.py[  95] : INFO  0-10 | lr : 0.000029 | val_loss : 1.7402 | val_roc_auc : 0.8169 | data_load_times : 47.00 | batch_run_times : 47.58\n",
      "[2022-12-07 14:00:08] 1068281035.py[  95] : INFO  0-11 | lr : 0.000021 | val_loss : 1.7228 | val_roc_auc : 0.8321 | data_load_times : 47.81 | batch_run_times : 48.35\n",
      "[2022-12-07 14:00:54] 1068281035.py[  95] : INFO  0-12 | lr : 0.000014 | val_loss : 1.7142 | val_roc_auc : 0.8313 | data_load_times : 48.15 | batch_run_times : 48.72\n",
      "[2022-12-07 14:01:41] 1068281035.py[  95] : INFO  0-13 | lr : 0.000009 | val_loss : 1.6888 | val_roc_auc : 0.8341 | data_load_times : 47.09 | batch_run_times : 47.67\n",
      "[2022-12-07 14:02:27] 1068281035.py[  95] : INFO  0-14 | lr : 0.000006 | val_loss : 1.7104 | val_roc_auc : 0.8233 | data_load_times : 45.61 | batch_run_times : 46.20\n",
      "[2022-12-07 14:03:14] 1068281035.py[  95] : INFO  0-15 | lr : 0.000100 | val_loss : 1.7655 | val_roc_auc : 0.8104 | data_load_times : 47.31 | batch_run_times : 47.90\n",
      "[2022-12-07 14:04:00] 1068281035.py[  95] : INFO  0-16 | lr : 0.000100 | val_loss : 1.7407 | val_roc_auc : 0.8115 | data_load_times : 46.94 | batch_run_times : 47.50\n",
      "[2022-12-07 14:04:45] 1068281035.py[  95] : INFO  0-17 | lr : 0.000100 | val_loss : 1.6644 | val_roc_auc : 0.8486 | data_load_times : 45.21 | batch_run_times : 45.75\n",
      "[2022-12-07 14:05:31] 1068281035.py[  95] : INFO  0-18 | lr : 0.000099 | val_loss : 1.7122 | val_roc_auc : 0.7991 | data_load_times : 44.80 | batch_run_times : 45.36\n",
      "[2022-12-07 14:06:17] 1068281035.py[  95] : INFO  0-19 | lr : 0.000099 | val_loss : 1.6525 | val_roc_auc : 0.8296 | data_load_times : 45.62 | batch_run_times : 46.24\n",
      "[2022-12-07 14:07:04] 1068281035.py[  95] : INFO  0-20 | lr : 0.000098 | val_loss : 1.6238 | val_roc_auc : 0.8271 | data_load_times : 45.31 | batch_run_times : 45.87\n",
      "[2022-12-07 14:07:49] 1068281035.py[  95] : INFO  0-21 | lr : 0.000098 | val_loss : 1.5462 | val_roc_auc : 0.8232 | data_load_times : 46.74 | batch_run_times : 47.33\n",
      "[2022-12-07 14:08:36] 1068281035.py[  95] : INFO  0-22 | lr : 0.000097 | val_loss : 1.6007 | val_roc_auc : 0.8299 | data_load_times : 45.60 | batch_run_times : 46.21\n",
      "[2022-12-07 14:09:22] 1068281035.py[  95] : INFO  0-23 | lr : 0.000096 | val_loss : 1.5696 | val_roc_auc : 0.8165 | data_load_times : 46.58 | batch_run_times : 47.15\n",
      "[2022-12-07 14:10:09] 1068281035.py[  95] : INFO  0-24 | lr : 0.000095 | val_loss : 1.5042 | val_roc_auc : 0.8305 | data_load_times : 46.98 | batch_run_times : 47.53\n",
      "[2022-12-07 14:10:55] 1068281035.py[  95] : INFO  0-25 | lr : 0.000094 | val_loss : 1.6259 | val_roc_auc : 0.8133 | data_load_times : 44.93 | batch_run_times : 45.48\n",
      "[2022-12-07 14:11:40] 1068281035.py[  95] : INFO  0-26 | lr : 0.000092 | val_loss : 1.5788 | val_roc_auc : 0.8034 | data_load_times : 44.80 | batch_run_times : 45.35\n",
      "[2022-12-07 14:12:26] 1068281035.py[  95] : INFO  0-27 | lr : 0.000091 | val_loss : 1.5392 | val_roc_auc : 0.7967 | data_load_times : 46.96 | batch_run_times : 47.53\n",
      "[2022-12-07 14:13:12] 1068281035.py[  95] : INFO  0-28 | lr : 0.000089 | val_loss : 1.5205 | val_roc_auc : 0.8103 | data_load_times : 44.02 | batch_run_times : 44.69\n",
      "[2022-12-07 14:13:58] 1068281035.py[  95] : INFO  0-29 | lr : 0.000088 | val_loss : 1.4288 | val_roc_auc : 0.8236 | data_load_times : 44.82 | batch_run_times : 45.39\n",
      "[2022-12-07 14:14:44] 1068281035.py[  95] : INFO  0-30 | lr : 0.000086 | val_loss : 1.5453 | val_roc_auc : 0.7918 | data_load_times : 47.37 | batch_run_times : 47.96\n",
      "[2022-12-07 14:15:30] 1068281035.py[  95] : INFO  0-31 | lr : 0.000084 | val_loss : 1.4709 | val_roc_auc : 0.8415 | data_load_times : 44.31 | batch_run_times : 44.92\n",
      "[2022-12-07 14:16:16] 1068281035.py[  95] : INFO  0-32 | lr : 0.000082 | val_loss : 1.4968 | val_roc_auc : 0.8406 | data_load_times : 46.98 | batch_run_times : 47.51\n",
      "[2022-12-07 14:17:01] 1068281035.py[  95] : INFO  0-33 | lr : 0.000080 | val_loss : 1.4521 | val_roc_auc : 0.8291 | data_load_times : 45.51 | batch_run_times : 46.10\n",
      "[2022-12-07 14:17:48] 1068281035.py[  95] : INFO  0-34 | lr : 0.000078 | val_loss : 1.4694 | val_roc_auc : 0.8255 | data_load_times : 43.74 | batch_run_times : 44.32\n",
      "[2022-12-07 14:18:33] 1068281035.py[  95] : INFO  0-35 | lr : 0.000076 | val_loss : 1.4775 | val_roc_auc : 0.8040 | data_load_times : 45.94 | batch_run_times : 46.50\n",
      "[2022-12-07 14:19:19] 1068281035.py[  95] : INFO  0-36 | lr : 0.000074 | val_loss : 1.6155 | val_roc_auc : 0.7836 | data_load_times : 44.33 | batch_run_times : 44.95\n",
      "[2022-12-07 14:20:05] 1068281035.py[  95] : INFO  0-37 | lr : 0.000072 | val_loss : 1.5385 | val_roc_auc : 0.8040 | data_load_times : 45.81 | batch_run_times : 46.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-12-07 14:20:50] 1068281035.py[  95] : INFO  0-38 | lr : 0.000070 | val_loss : 1.5005 | val_roc_auc : 0.7929 | data_load_times : 46.10 | batch_run_times : 46.68\n",
      "[2022-12-07 14:21:36] 1068281035.py[  95] : INFO  0-39 | lr : 0.000067 | val_loss : 1.4513 | val_roc_auc : 0.8188 | data_load_times : 44.48 | batch_run_times : 45.03\n",
      "[2022-12-07 14:22:22] 1068281035.py[  95] : INFO  0-40 | lr : 0.000065 | val_loss : 1.4957 | val_roc_auc : 0.7991 | data_load_times : 48.77 | batch_run_times : 49.35\n",
      "Trainer was signaled to stop but required minimum epochs (75) or minimum steps (None) has not been met. Training will continue...\n",
      "[2022-12-07 14:23:09] 1068281035.py[  95] : INFO  0-41 | lr : 0.000062 | val_loss : 1.4941 | val_roc_auc : 0.7841 | data_load_times : 48.01 | batch_run_times : 48.56\n",
      "Trainer was signaled to stop but required minimum epochs (75) or minimum steps (None) has not been met. Training will continue...\n",
      "[2022-12-07 14:23:55] 1068281035.py[  95] : INFO  0-42 | lr : 0.000060 | val_loss : 1.5060 | val_roc_auc : 0.7873 | data_load_times : 46.37 | batch_run_times : 46.91\n",
      "Trainer was signaled to stop but required minimum epochs (75) or minimum steps (None) has not been met. Training will continue...\n",
      "[2022-12-07 14:24:40] 1068281035.py[  95] : INFO  0-43 | lr : 0.000057 | val_loss : 1.6678 | val_roc_auc : 0.7867 | data_load_times : 44.98 | batch_run_times : 45.59\n",
      "Trainer was signaled to stop but required minimum epochs (75) or minimum steps (None) has not been met. Training will continue...\n",
      "[2022-12-07 14:25:25] 1068281035.py[  95] : INFO  0-44 | lr : 0.000055 | val_loss : 1.5710 | val_roc_auc : 0.7620 | data_load_times : 46.03 | batch_run_times : 46.76\n",
      "Trainer was signaled to stop but required minimum epochs (75) or minimum steps (None) has not been met. Training will continue...\n",
      "[2022-12-07 14:26:11] 1068281035.py[  95] : INFO  0-45 | lr : 0.000053 | val_loss : 1.6396 | val_roc_auc : 0.7768 | data_load_times : 47.31 | batch_run_times : 47.94\n",
      "Trainer was signaled to stop but required minimum epochs (75) or minimum steps (None) has not been met. Training will continue...\n",
      "[2022-12-07 14:26:58] 1068281035.py[  95] : INFO  0-46 | lr : 0.000050 | val_loss : 1.7227 | val_roc_auc : 0.7384 | data_load_times : 46.15 | batch_run_times : 46.68\n",
      "Trainer was signaled to stop but required minimum epochs (75) or minimum steps (None) has not been met. Training will continue...\n",
      "[2022-12-07 14:27:43] 1068281035.py[  95] : INFO  0-47 | lr : 0.000048 | val_loss : 1.5672 | val_roc_auc : 0.8037 | data_load_times : 47.30 | batch_run_times : 47.87\n",
      "Trainer was signaled to stop but required minimum epochs (75) or minimum steps (None) has not been met. Training will continue...\n",
      "[2022-12-07 14:28:29] 1068281035.py[  95] : INFO  0-48 | lr : 0.000045 | val_loss : 1.4836 | val_roc_auc : 0.7983 | data_load_times : 45.44 | batch_run_times : 46.02\n",
      "Trainer was signaled to stop but required minimum epochs (75) or minimum steps (None) has not been met. Training will continue...\n",
      "[2022-12-07 14:29:14] 1068281035.py[  95] : INFO  0-49 | lr : 0.000043 | val_loss : 1.5606 | val_roc_auc : 0.7655 | data_load_times : 45.72 | batch_run_times : 46.29\n",
      "Trainer was signaled to stop but required minimum epochs (75) or minimum steps (None) has not been met. Training will continue...\n",
      "[2022-12-07 14:30:01] 1068281035.py[  95] : INFO  0-50 | lr : 0.000040 | val_loss : 1.7306 | val_roc_auc : 0.7529 | data_load_times : 47.17 | batch_run_times : 47.83\n",
      "Trainer was signaled to stop but required minimum epochs (75) or minimum steps (None) has not been met. Training will continue...\n",
      "[2022-12-07 14:30:48] 1068281035.py[  95] : INFO  0-51 | lr : 0.000038 | val_loss : 1.5554 | val_roc_auc : 0.7739 | data_load_times : 46.57 | batch_run_times : 47.22\n",
      "Trainer was signaled to stop but required minimum epochs (75) or minimum steps (None) has not been met. Training will continue...\n",
      "[2022-12-07 14:31:34] 1068281035.py[  95] : INFO  0-52 | lr : 0.000035 | val_loss : 1.6363 | val_roc_auc : 0.7553 | data_load_times : 44.98 | batch_run_times : 45.54\n",
      "Trainer was signaled to stop but required minimum epochs (75) or minimum steps (None) has not been met. Training will continue...\n",
      "[2022-12-07 14:32:20] 1068281035.py[  95] : INFO  0-53 | lr : 0.000033 | val_loss : 1.5784 | val_roc_auc : 0.7821 | data_load_times : 48.37 | batch_run_times : 48.93\n",
      "Trainer was signaled to stop but required minimum epochs (75) or minimum steps (None) has not been met. Training will continue...\n",
      "[2022-12-07 14:33:06] 1068281035.py[  95] : INFO  0-54 | lr : 0.000031 | val_loss : 1.7353 | val_roc_auc : 0.7601 | data_load_times : 46.79 | batch_run_times : 47.43\n",
      "Trainer was signaled to stop but required minimum epochs (75) or minimum steps (None) has not been met. Training will continue...\n",
      "[2022-12-07 14:33:52] 1068281035.py[  95] : INFO  0-55 | lr : 0.000029 | val_loss : 1.6797 | val_roc_auc : 0.7618 | data_load_times : 46.66 | batch_run_times : 47.22\n",
      "Trainer was signaled to stop but required minimum epochs (75) or minimum steps (None) has not been met. Training will continue...\n",
      "[2022-12-07 14:34:38] 1068281035.py[  95] : INFO  0-56 | lr : 0.000027 | val_loss : 1.7047 | val_roc_auc : 0.7545 | data_load_times : 45.87 | batch_run_times : 46.46\n",
      "Trainer was signaled to stop but required minimum epochs (75) or minimum steps (None) has not been met. Training will continue...\n",
      "[2022-12-07 14:35:23] 1068281035.py[  95] : INFO  0-57 | lr : 0.000025 | val_loss : 1.6947 | val_roc_auc : 0.7519 | data_load_times : 46.13 | batch_run_times : 46.82\n",
      "Trainer was signaled to stop but required minimum epochs (75) or minimum steps (None) has not been met. Training will continue...\n",
      "[2022-12-07 14:36:08] 1068281035.py[  95] : INFO  0-58 | lr : 0.000023 | val_loss : 1.7077 | val_roc_auc : 0.7808 | data_load_times : 42.97 | batch_run_times : 43.59\n",
      "Trainer was signaled to stop but required minimum epochs (75) or minimum steps (None) has not been met. Training will continue...\n",
      "[2022-12-07 14:36:54] 1068281035.py[  95] : INFO  0-59 | lr : 0.000021 | val_loss : 1.6585 | val_roc_auc : 0.7472 | data_load_times : 45.22 | batch_run_times : 45.79\n",
      "Trainer was signaled to stop but required minimum epochs (75) or minimum steps (None) has not been met. Training will continue...\n",
      "[2022-12-07 14:37:39] 1068281035.py[  95] : INFO  0-60 | lr : 0.000019 | val_loss : 1.6400 | val_roc_auc : 0.7844 | data_load_times : 43.00 | batch_run_times : 43.59\n",
      "Trainer was signaled to stop but required minimum epochs (75) or minimum steps (None) has not been met. Training will continue...\n",
      "[2022-12-07 14:38:25] 1068281035.py[  95] : INFO  0-61 | lr : 0.000017 | val_loss : 1.6940 | val_roc_auc : 0.7497 | data_load_times : 45.84 | batch_run_times : 46.49\n",
      "Trainer was signaled to stop but required minimum epochs (75) or minimum steps (None) has not been met. Training will continue...\n",
      "[2022-12-07 14:39:11] 1068281035.py[  95] : INFO  0-62 | lr : 0.000016 | val_loss : 1.7207 | val_roc_auc : 0.7536 | data_load_times : 49.37 | batch_run_times : 50.03\n",
      "Trainer was signaled to stop but required minimum epochs (75) or minimum steps (None) has not been met. Training will continue...\n",
      "[2022-12-07 14:39:56] 1068281035.py[  95] : INFO  0-63 | lr : 0.000014 | val_loss : 1.6841 | val_roc_auc : 0.7664 | data_load_times : 46.24 | batch_run_times : 46.82\n",
      "Trainer was signaled to stop but required minimum epochs (75) or minimum steps (None) has not been met. Training will continue...\n",
      "[2022-12-07 14:40:42] 1068281035.py[  95] : INFO  0-64 | lr : 0.000013 | val_loss : 1.7584 | val_roc_auc : 0.7511 | data_load_times : 46.78 | batch_run_times : 47.36\n",
      "Trainer was signaled to stop but required minimum epochs (75) or minimum steps (None) has not been met. Training will continue...\n",
      "[2022-12-07 14:41:28] 1068281035.py[  95] : INFO  0-65 | lr : 0.000011 | val_loss : 1.7423 | val_roc_auc : 0.7614 | data_load_times : 46.72 | batch_run_times : 47.27\n",
      "Trainer was signaled to stop but required minimum epochs (75) or minimum steps (None) has not been met. Training will continue...\n",
      "[2022-12-07 14:42:14] 1068281035.py[  95] : INFO  0-66 | lr : 0.000010 | val_loss : 1.6979 | val_roc_auc : 0.7434 | data_load_times : 45.13 | batch_run_times : 45.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer was signaled to stop but required minimum epochs (75) or minimum steps (None) has not been met. Training will continue...\n",
      "[2022-12-07 14:43:00] 1068281035.py[  95] : INFO  0-67 | lr : 0.000009 | val_loss : 1.7378 | val_roc_auc : 0.7726 | data_load_times : 34.60 | batch_run_times : 35.21\n",
      "Trainer was signaled to stop but required minimum epochs (75) or minimum steps (None) has not been met. Training will continue...\n",
      "[2022-12-07 14:43:45] 1068281035.py[  95] : INFO  0-68 | lr : 0.000008 | val_loss : 1.7063 | val_roc_auc : 0.7604 | data_load_times : 46.78 | batch_run_times : 47.34\n",
      "Trainer was signaled to stop but required minimum epochs (75) or minimum steps (None) has not been met. Training will continue...\n",
      "[2022-12-07 14:44:31] 1068281035.py[  95] : INFO  0-69 | lr : 0.000007 | val_loss : 1.6857 | val_roc_auc : 0.7464 | data_load_times : 46.21 | batch_run_times : 46.83\n",
      "Trainer was signaled to stop but required minimum epochs (75) or minimum steps (None) has not been met. Training will continue...\n",
      "[2022-12-07 14:45:16] 1068281035.py[  95] : INFO  0-70 | lr : 0.000007 | val_loss : 1.7661 | val_roc_auc : 0.7541 | data_load_times : 46.67 | batch_run_times : 47.35\n",
      "Trainer was signaled to stop but required minimum epochs (75) or minimum steps (None) has not been met. Training will continue...\n",
      "[2022-12-07 14:46:02] 1068281035.py[  95] : INFO  0-71 | lr : 0.000006 | val_loss : 1.6403 | val_roc_auc : 0.7712 | data_load_times : 46.30 | batch_run_times : 46.85\n",
      "Trainer was signaled to stop but required minimum epochs (75) or minimum steps (None) has not been met. Training will continue...\n",
      "[2022-12-07 14:46:48] 1068281035.py[  95] : INFO  0-72 | lr : 0.000006 | val_loss : 1.8094 | val_roc_auc : 0.7514 | data_load_times : 46.12 | batch_run_times : 46.68\n",
      "Trainer was signaled to stop but required minimum epochs (75) or minimum steps (None) has not been met. Training will continue...\n",
      "[2022-12-07 14:47:33] 1068281035.py[  95] : INFO  0-73 | lr : 0.000005 | val_loss : 1.7953 | val_roc_auc : 0.7400 | data_load_times : 44.52 | batch_run_times : 45.12\n",
      "Trainer was signaled to stop but required minimum epochs (75) or minimum steps (None) has not been met. Training will continue...\n",
      "[2022-12-07 14:48:20] 1068281035.py[  95] : INFO  0-74 | lr : 0.000005 | val_loss : 1.7381 | val_roc_auc : 0.7553 | data_load_times : 46.20 | batch_run_times : 46.80\n",
      "Epoch 00075: early stopping triggered.\n",
      "[2022-12-07 14:48:20] 866901696.py[  61] : INFO  Fold 1 num train records: 199\n",
      "[2022-12-07 14:48:20] 866901696.py[  62] : INFO  Fold 1 num val records: 100\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "Using native 16bit precision.\n",
      "[2022-12-07 14:49:08] 1068281035.py[  95] : INFO  1-0 | lr : 0.000100 | val_loss : 2.0436 | val_roc_auc : 0.7232 | data_load_times : 46.60 | batch_run_times : 47.15\n",
      "[2022-12-07 14:49:56] 1068281035.py[  95] : INFO  1-1 | lr : 0.000099 | val_loss : 1.9811 | val_roc_auc : 0.7780 | data_load_times : 48.00 | batch_run_times : 48.97\n",
      "[2022-12-07 14:50:44] 1068281035.py[  95] : INFO  1-2 | lr : 0.000096 | val_loss : 1.9507 | val_roc_auc : 0.7945 | data_load_times : 46.96 | batch_run_times : 47.90\n",
      "[2022-12-07 14:51:33] 1068281035.py[  95] : INFO  1-3 | lr : 0.000091 | val_loss : 1.9259 | val_roc_auc : 0.7951 | data_load_times : 45.68 | batch_run_times : 46.28\n",
      "[2022-12-07 14:52:21] 1068281035.py[  95] : INFO  1-4 | lr : 0.000084 | val_loss : 1.8497 | val_roc_auc : 0.8374 | data_load_times : 43.87 | batch_run_times : 44.45\n",
      "[2022-12-07 14:53:10] 1068281035.py[  95] : INFO  1-5 | lr : 0.000076 | val_loss : 1.8440 | val_roc_auc : 0.8093 | data_load_times : 45.35 | batch_run_times : 45.89\n",
      "[2022-12-07 14:53:59] 1068281035.py[  95] : INFO  1-6 | lr : 0.000067 | val_loss : 1.8154 | val_roc_auc : 0.8054 | data_load_times : 48.09 | batch_run_times : 48.87\n",
      "[2022-12-07 14:54:48] 1068281035.py[  95] : INFO  1-7 | lr : 0.000057 | val_loss : 1.7753 | val_roc_auc : 0.8205 | data_load_times : 48.16 | batch_run_times : 48.71\n",
      "[2022-12-07 14:55:37] 1068281035.py[  95] : INFO  1-8 | lr : 0.000048 | val_loss : 1.7717 | val_roc_auc : 0.8312 | data_load_times : 46.14 | batch_run_times : 46.77\n",
      "[2022-12-07 14:56:26] 1068281035.py[  95] : INFO  1-9 | lr : 0.000038 | val_loss : 1.7599 | val_roc_auc : 0.8306 | data_load_times : 45.40 | batch_run_times : 45.96\n",
      "[2022-12-07 14:57:14] 1068281035.py[  95] : INFO  1-10 | lr : 0.000029 | val_loss : 1.7194 | val_roc_auc : 0.8385 | data_load_times : 44.86 | batch_run_times : 45.55\n",
      "[2022-12-07 14:58:02] 1068281035.py[  95] : INFO  1-11 | lr : 0.000021 | val_loss : 1.6922 | val_roc_auc : 0.8400 | data_load_times : 45.64 | batch_run_times : 46.21\n",
      "[2022-12-07 14:58:51] 1068281035.py[  95] : INFO  1-12 | lr : 0.000014 | val_loss : 1.6892 | val_roc_auc : 0.8497 | data_load_times : 47.32 | batch_run_times : 47.89\n",
      "[2022-12-07 14:59:40] 1068281035.py[  95] : INFO  1-13 | lr : 0.000009 | val_loss : 1.6854 | val_roc_auc : 0.8543 | data_load_times : 47.52 | batch_run_times : 48.22\n",
      "[2022-12-07 15:00:29] 1068281035.py[  95] : INFO  1-14 | lr : 0.000006 | val_loss : 1.6874 | val_roc_auc : 0.8431 | data_load_times : 47.79 | batch_run_times : 48.37\n",
      "[2022-12-07 15:01:18] 1068281035.py[  95] : INFO  1-15 | lr : 0.000100 | val_loss : 1.7386 | val_roc_auc : 0.8220 | data_load_times : 46.30 | batch_run_times : 46.92\n",
      "[2022-12-07 15:02:07] 1068281035.py[  95] : INFO  1-16 | lr : 0.000100 | val_loss : 1.7048 | val_roc_auc : 0.8064 | data_load_times : 45.55 | batch_run_times : 46.10\n",
      "[2022-12-07 15:02:56] 1068281035.py[  95] : INFO  1-17 | lr : 0.000100 | val_loss : 1.6981 | val_roc_auc : 0.8029 | data_load_times : 45.77 | batch_run_times : 46.37\n",
      "[2022-12-07 15:03:44] 1068281035.py[  95] : INFO  1-18 | lr : 0.000099 | val_loss : 1.7254 | val_roc_auc : 0.8010 | data_load_times : 45.98 | batch_run_times : 46.57\n",
      "[2022-12-07 15:04:31] 1068281035.py[  95] : INFO  1-19 | lr : 0.000099 | val_loss : 1.6667 | val_roc_auc : 0.8238 | data_load_times : 44.60 | batch_run_times : 45.23\n",
      "[2022-12-07 15:05:20] 1068281035.py[  95] : INFO  1-20 | lr : 0.000098 | val_loss : 1.6343 | val_roc_auc : 0.8157 | data_load_times : 45.65 | batch_run_times : 46.27\n",
      "[2022-12-07 15:06:10] 1068281035.py[  95] : INFO  1-21 | lr : 0.000098 | val_loss : 1.6057 | val_roc_auc : 0.8043 | data_load_times : 45.85 | batch_run_times : 46.46\n",
      "[2022-12-07 15:06:59] 1068281035.py[  95] : INFO  1-22 | lr : 0.000097 | val_loss : 1.5563 | val_roc_auc : 0.8263 | data_load_times : 46.45 | batch_run_times : 47.05\n",
      "[2022-12-07 15:07:48] 1068281035.py[  95] : INFO  1-23 | lr : 0.000096 | val_loss : 1.6424 | val_roc_auc : 0.7953 | data_load_times : 46.53 | batch_run_times : 47.21\n",
      "[2022-12-07 15:08:35] 1068281035.py[  95] : INFO  1-24 | lr : 0.000095 | val_loss : 1.6375 | val_roc_auc : 0.7932 | data_load_times : 43.78 | batch_run_times : 44.59\n",
      "[2022-12-07 15:09:23] 1068281035.py[  95] : INFO  1-25 | lr : 0.000094 | val_loss : 1.6054 | val_roc_auc : 0.7916 | data_load_times : 45.51 | batch_run_times : 46.17\n"
     ]
    }
   ],
   "source": [
    "# Initialise hyperparameters\n",
    "hparams = init_hparams()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "log_notes = \"t_mult increased to 4 from 2\"\n",
    "\n",
    "# Initialise logger\n",
    "logger = init_logger(hparams.log_name, hparams.log_dir)\n",
    "\n",
    "# Log parameters\n",
    "logger.info(f\"backbone: {hparams.backbone}\")\n",
    "logger.info(f\"device_name: {hparams.device_name}\")\n",
    "logger.info(f\"gpus: {hparams.gpus}\")\n",
    "logger.info(f\"n_workers: {hparams.n_workers}\")\n",
    "logger.info(f\"image_size: {hparams.image_size}\")\n",
    "logger.info(f\"seed: {hparams.seed}\")\n",
    "logger.info(f\"min_epochs: {hparams.min_epochs}\")\n",
    "logger.info(f\"max_epochs: {hparams.max_epochs}\")\n",
    "logger.info(f\"patience: {hparams.patience}\")\n",
    "logger.info(f\"train_batch_size: {hparams.train_batch_size}\")\n",
    "logger.info(f\"val_batch_size: {hparams.val_batch_size}\")\n",
    "logger.info(f\"n_splits: {hparams.n_splits}\")\n",
    "logger.info(f\"test_size: {hparams.test_size}\")\n",
    "logger.info(f\"learning rate: {hparams.lr}\")\n",
    "logger.info(f\"weight_decay: {hparams.weight_decay}\")\n",
    "logger.info(f\"epsilon: {hparams.epsilon}\")\n",
    "logger.info(f\"amsgrad: {hparams.amsgrad}\")\n",
    "logger.info(f\"betas: {hparams.betas}\")\n",
    "logger.info(f\"precision: {hparams.precision}\")\n",
    "logger.info(f\"gradient_clip_val: {hparams.gradient_clip_val}\")\n",
    "logger.info(f\"eta_min: {hparams.eta_min}\")\n",
    "logger.info(f\"t_max: {hparams.t_max}\")\n",
    "logger.info(f\"t_mult: {hparams.t_mult}\")\n",
    "logger.info(f\"log_dir: {hparams.log_dir}\")\n",
    "logger.info(f\"log_name: {hparams.log_name}\")\n",
    "\n",
    "# Log any notes if they exist\n",
    "if \"log_notes\" in locals():\n",
    "    logger.info(f\"Notes: {log_notes}\")\n",
    "\n",
    "\n",
    "# Create transform pipeline\n",
    "transforms = generate_transforms(hparams.image_size)\n",
    "\n",
    "# List for validation scores \n",
    "val_loss_scores = []\n",
    "\n",
    "# Initialise cross validation\n",
    "folds = StratifiedKFold(n_splits=hparams.n_splits, shuffle=True, random_state=hparams.seed)\n",
    "\n",
    "# Start cross validation\n",
    "for fold_i, (train_index, val_index) in enumerate(folds.split(metadata[[\"img_path\"]], metadata[[\"label\"]])):\n",
    "    hparams.fold_i = fold_i\n",
    "    # Split train images and validation sets\n",
    "    train_data = metadata.iloc[train_index][[\"img_path\", \"label\"]].reset_index(drop=True)\n",
    "    train_data = pd.get_dummies(train_data, columns=[\"label\"], prefix=\"\", prefix_sep=\"\")\n",
    "\n",
    "    val_data = metadata.iloc[val_index][[\"img_path\", \"label\"]].reset_index(drop=True)\n",
    "    val_data = pd.get_dummies(val_data, columns=[\"label\"], prefix=\"\", prefix_sep=\"\")\n",
    "    \n",
    "    logger.info(f\"Fold {fold_i} num train records: {train_data.shape[0]}\")\n",
    "    logger.info(f\"Fold {fold_i} num val records: {val_data.shape[0]}\")\n",
    "    \n",
    "    train_dataloader, val_dataloader = generate_dataloaders(hparams, train_data, val_data, transforms)\n",
    "    \n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        save_top_k=2,\n",
    "        mode=\"min\",\n",
    "        filepath=os.path.join(\n",
    "            hparams.log_dir, \n",
    "            hparams.log_name, \n",
    "            f\"fold={fold_i}\" + \"-{epoch}-{val_loss:.4f}-{val_roc_auc:.4f}\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor=\"val_loss\", \n",
    "        patience=hparams.patience, \n",
    "        mode=\"min\", \n",
    "        verbose=hparams.verbose\n",
    "    )\n",
    "    \n",
    "    # Instance Model, Trainer and train model\n",
    "    model = CoolSystem(hparams)\n",
    "    trainer = pl.Trainer(\n",
    "        gpus=hparams.gpus,\n",
    "        min_epochs=hparams.min_epochs,\n",
    "        max_epochs=hparams.max_epochs,\n",
    "        early_stop_callback=early_stop_callback,\n",
    "        checkpoint_callback=checkpoint_callback,\n",
    "        progress_bar_refresh_rate=0,\n",
    "        precision=hparams.precision,\n",
    "        num_sanity_val_steps=0,\n",
    "        profiler=False,\n",
    "        weights_summary=None,\n",
    "        gradient_clip_val=hparams.gradient_clip_val,\n",
    "        default_root_dir=os.path.join(hparams.log_dir, hparams.log_name)\n",
    "    )\n",
    "    \n",
    "    # Fit model\n",
    "    trainer.fit(model, train_dataloader, val_dataloader)\n",
    "            \n",
    "    # Save val scores\n",
    "    val_loss_scores.append(checkpoint_callback.best)\n",
    "    \n",
    "    # Cleanup\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "val_loss_scores = [i.item() for i in val_loss_scores]\n",
    "\n",
    "# Add val scores to csv with all scores\n",
    "if os.path.isfile(\"../logs/scores.csv\") == False:\n",
    "    pd.DataFrame(columns=[\"name\", \"scores\", \"mean_score\"]).to_csv(\"../logs/scores.csv\", index=False)\n",
    "    \n",
    "# Append to current scores csv\n",
    "all_scores_df = pd.concat([\n",
    "    pd.read_csv(\"../logs/scores.csv\"),\n",
    "    pd.DataFrame.from_dict(\n",
    "        {\n",
    "            \"name\": [hparams.log_name],\n",
    "            \"scores\": [val_loss_scores],\n",
    "            \"mean_score\": [np.mean(val_loss_scores)]\n",
    "        }\n",
    "    )],\n",
    "    ignore_index=True\n",
    ")\n",
    "# Write all scores df to csv\n",
    "all_scores_df.to_csv(\"../logs/scores.csv\", index=False)\n",
    "\n",
    "logger.info(f\"Best scores: {val_loss_scores}\")\n",
    "logger.info(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aee771",
   "metadata": {},
   "source": [
    "## Validation Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7feb0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model run path and define chosen fold\n",
    "log_dir = \"../logs/logs\"\n",
    "#model_run = \"2022_11_08_14:57:52\" # manually choose model path\n",
    "model_run = hparams.log_name\n",
    "model_run_path = os.path.join(log_dir, model_run)\n",
    "#best_fold = 1 # manually choose model path\n",
    "best_fold = val_loss_scores.index(min(val_loss_scores))\n",
    "\n",
    "# Get best model for chosen fold\n",
    "model_run_dir = os.listdir(model_run_path)\n",
    "model_folds = [i for i in model_run_dir if i.startswith(f\"fold={best_fold}\")]\n",
    "model_folds_scores = [float(i.split(\"val_loss=\")[1].split(\"-\")[0]) for i in model_folds]\n",
    "model_name = model_folds[model_folds_scores.index(min(model_folds_scores))]\n",
    "model_path = os.path.join(model_run_path, model_name)\n",
    "\n",
    "# Load fold's model\n",
    "model = CoolSystem(hparams)\n",
    "model.load_state_dict(\n",
    "    torch.load(model_path)[\"state_dict\"]\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Retrieve validation indices for chosen fold\n",
    "for fold_i, (train_index, val_index) in enumerate(folds.split(metadata[[\"img_path\"]], metadata[[\"label\"]])):\n",
    "    if fold_i == best_fold:\n",
    "        break\n",
    "\n",
    "# Select fold validation images\n",
    "X_val = torch.from_numpy(X_train[val_index]).permute(0, 3, 1, 2).float()\n",
    "\n",
    "# Create predictions looped by batch\n",
    "counter = 0\n",
    "val_i_batch = []\n",
    "val_idx_batch = []\n",
    "scores_df = pd.DataFrame()\n",
    "\n",
    "for i, idx in tqdm(enumerate(val_index)):\n",
    "    counter += 1\n",
    "    val_i_batch.append(i) # arrays don't preserve index so need ordered index values\n",
    "    val_idx_batch.append(idx) # for preserved index\n",
    "    \n",
    "    # Run inference for val_batch_size\n",
    "    if counter == hparams.val_batch_size:\n",
    "        preds = model(X_val[val_i_batch])\n",
    "        \n",
    "        # Create activation output\n",
    "        log_softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        # Convert raw output to probabilities\n",
    "        preds = np.exp(log_softmax(preds).detach().numpy())\n",
    "\n",
    "        # Create df with img paths and predicted label probs\n",
    "        scores_df_batch = pd.DataFrame(preds, columns=val_data.columns[1:])\n",
    "        scores_df_batch = pd.merge(\n",
    "            metadata.iloc[val_idx_batch, 1:3].reset_index(drop=True),\n",
    "            scores_df_batch, \n",
    "            left_index=True,\n",
    "            right_index=True\n",
    "        )\n",
    "        scores_df = pd.concat([scores_df, scores_df_batch], ignore_index=True, axis=0)\n",
    "\n",
    "        # Cleanup\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        # Reset counter and batch\n",
    "        counter = 0\n",
    "        val_i_batch = []\n",
    "        val_idx_batch = []\n",
    "        \n",
    "    # Run inference for remaining batch\n",
    "    elif idx == val_index[-1]:\n",
    "        preds = model(X_val[val_i_batch])\n",
    "        \n",
    "        # Create activation output\n",
    "        log_softmax = torch.nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        # Convert raw output to probabilities\n",
    "        preds = np.exp(log_softmax(preds).detach().numpy())\n",
    "\n",
    "        # Create df with img paths and predicted label probs\n",
    "        scores_df_batch = pd.DataFrame(preds, columns=val_data.columns[1:])\n",
    "        scores_df_batch = pd.merge(\n",
    "            metadata.iloc[val_idx_batch, 1:3].reset_index(drop=True),\n",
    "            scores_df_batch, \n",
    "            left_index=True,\n",
    "            right_index=True\n",
    "        )\n",
    "        scores_df = pd.concat([scores_df, scores_df_batch], ignore_index=True, axis=0)\n",
    "\n",
    "        # Cleanup\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        \n",
    "# Write predictions to log\n",
    "scores_df.to_csv(\n",
    "    os.path.join(model_run_path, f\"{model_run}_preds_fold_{best_fold}.csv\"),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc3edc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a910bef3",
   "metadata": {},
   "source": [
    "## Validation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b4293f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(scores_df['img_path'].unique())} unique image paths.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6565caec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation label counts:\")\n",
    "print(scores_df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f8676c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation prediction counts:\")\n",
    "print(\n",
    "    pd.melt(\n",
    "        scores_df,\n",
    "        id_vars=[\"img_path\", \"label\"],\n",
    "        value_vars=[\"ant\", \"bedbug\", \"bee\", \"horsefly\", \"mite\", \"mosquito\" ,\"none\", \"tick\"],\n",
    "        var_name=\"pred_label\",\n",
    "        value_name=\"pred_prob\"\n",
    "    ).sort_values([\"img_path\", \"pred_prob\"], ascending=False) \\\n",
    "    .groupby([\"img_path\", \"label\"]).first()[\"pred_label\"] \\\n",
    "    .value_counts()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bf48d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability stats by label\n",
    "pd.concat(\n",
    "    [\n",
    "        pd.DataFrame(scores_df.iloc[:, 2:].mean(), columns=[\"mean\"]),\n",
    "        pd.DataFrame(scores_df.iloc[:, 2:].std(), columns=[\"std\"]),\n",
    "        pd.DataFrame(scores_df.iloc[:, 2:].min(), columns=[\"min\"]),\n",
    "        pd.DataFrame(scores_df.iloc[:, 2:].quantile(0.25)),\n",
    "        pd.DataFrame(scores_df.iloc[:, 2:].median(), columns=[\"median\"]),\n",
    "        pd.DataFrame(scores_df.iloc[:, 2:].quantile(0.75)),\n",
    "        pd.DataFrame(scores_df.iloc[:, 2:].max(), columns=[\"max\"]),\n",
    "        pd.DataFrame(scores_df.iloc[:, 2:].max() - scores_df.iloc[:, 2:].min(), columns=[\"range\"])\n",
    "    ], \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d47fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.melt(\n",
    "    scores_df,\n",
    "    id_vars=[\"img_path\", \"label\"],\n",
    "    value_vars=[\"ant\", \"bedbug\", \"bee\", \"horsefly\", \"mite\", \"mosquito\" ,\"none\", \"tick\"],\n",
    "    var_name=\"pred_label\",\n",
    "    value_name=\"pred_prob\"\n",
    ").pivot_table(\n",
    "    index=[\"label\"],\n",
    "    columns=[\"pred_label\"],\n",
    "    aggfunc=\"mean\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980e4931",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.melt(\n",
    "    scores_df,\n",
    "    id_vars=[\"img_path\", \"label\"],\n",
    "    value_vars=[\"ant\", \"bedbug\", \"bee\", \"horsefly\", \"mite\", \"mosquito\" ,\"none\", \"tick\"],\n",
    "    var_name=\"pred_label\",\n",
    "    value_name=\"pred_prob\"\n",
    ").sort_values([\"img_path\", \"pred_prob\"], ascending=False).groupby([\"img_path\", \"label\"]).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2249a0ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613be570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88807ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biteme",
   "language": "python",
   "name": "biteme"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
